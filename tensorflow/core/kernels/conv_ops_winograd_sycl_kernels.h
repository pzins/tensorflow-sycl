#ifndef TENSORFLOW_USE_SYCL
#error This file should only be included when compiling with SYCL support
#endif

#ifndef TENSORFLOW_KERNELS_CONV_OPS_WINOGRAD_SYCL_KERNELS_H_
#define TENSORFLOW_KERNELS_CONV_OPS_WINOGRAD_SYCL_KERNELS_H_

namespace tensorflow {
namespace winograd {

template <typename T, int M, int N, int R, int S>
struct InputTile {
  static constexpr int A = M + R - 1;
  static constexpr int B = N + S - 1;
  /**
   * Read the input data from the provided input array. The pointer is assumed
   * to be at the first value that should be read into the input tile.
   *
   * The input is expected to be in the NHWC data format.
   *
   * NOTE: The template here allows different address space attributes to be
   * passed with the pointer, rather than specifying the pointer will be to
   * global memory or to local memory.
   */
  template <typename _T>
  inline TF_ATTRIBUTE_ALWAYS_INLINE InputTile(_T* input, int n_cols,
                                              int n_channels, int rsize,
                                              int csize, int firstr,
                                              int firstc) {
    for (int r = 0; r < A; ++r) {
      for (int c = 0; c < B; ++c) {
        data[r][c] = (r < firstr || c < firstc || r >= rsize || c >= csize)
                         ? static_cast<_T>(0)
                         : input[(r * n_cols + c) * n_channels];
      }
    }
  }
  T data[A][B];
};
template <typename T, int M, int N, int R, int S>
struct BaseFilterTile {
  T data[R][S];
};
template <typename T, int M, int N, int R, int S, ConvType CType>
struct FilterTile;
template <typename T, int M, int N, int R, int S>
struct FilterTile<T, M, N, R, S, ConvType::Forward> final
    : public BaseFilterTile<T, M, N, R, S> {
  using BaseFilterTile<T, M, N, R, S>::data;
  /**
   * Read the filter data from the provided input array. The pointer is assumed
   * to be at the start of the filter tensor.
   *
   * The input is expected to be in (Height x Width x Channel x Feature) format.
   * The height of the filter (no. of rows) is expected to be R, and the width
   * (no. of cols) is S.
   *
   * NOTE: The template here allows different address space attributes to be
   * passed with the pointer, rather than specifying the pointer will be to
   * global memory or to local memory.
   */
  template <typename _T>
  inline TF_ATTRIBUTE_ALWAYS_INLINE FilterTile(_T const* input,
                                               int const channel,
                                               int const feature,
                                               int const n_channels,
                                               int const n_features) {
    input += channel * n_features + feature;
    for (int r = 0; r < R; ++r) {
      for (int c = 0; c < S; ++c) {
        int idx = (r * S + c) * n_channels * n_features;
        data[r][c] = input[idx];
      }
    }
  }
};
template <typename T, int M, int N, int R, int S>
struct FilterTile<T, M, N, R, S, ConvType::InputBackprop> final
    : public BaseFilterTile<T, M, N, R, S> {
  using BaseFilterTile<T, M, N, R, S>::data;
  /**
   * Read the filter data from the provided input array but mirror the filter
   * for use in backprop. The pointer is assumed to be at the start of the
   * filter tensor.
   *
   * The input is expected to be in (Height x Width x Channel x Feature) format.
   *
   * NOTE: The template here allows different address space attributes to be
   * passed with the pointer, rather than specifying the pointer will be to
   * global memory or to local memory.
   */
  template <typename _T>
  inline TF_ATTRIBUTE_ALWAYS_INLINE FilterTile(_T const* input,
                                               int const channel,
                                               int const feature,
                                               int const n_channels,
                                               int const n_features) {
    input += channel * n_features + feature;
    for (int r = 0; r < R; ++r) {
      for (int c = 0; c < S; ++c) {
        // Here the transforms (R - 1 - r) and (S - 1 - c) mirror the filter
        // data. Note that the channel and feature dims were switched in the
        // kernel params.
        int idx = ((R - 1 - r) * S + (S - 1 - c)) * n_channels * n_features;
        data[r][c] = input[idx];
      }
    }
  }
};
template <typename T, int M, int N, int R, int S>
struct FilterTile<T, M, N, R, S, ConvType::FilterBackprop> final
    : public BaseFilterTile<T, M, N, R, S> {
  using BaseFilterTile<T, M, N, R, S>::data;
  /**
   * Read the filter data from the provided input array.
   *
   * The input is expected to be in (Batch x Height x Width x Feature) format.
   *
   * NOTE: The template here allows different address space attributes to be
   * passed with the pointer, rather than specifying the pointer will be to
   * global memory or to local memory.
   */
  template <typename _T>
  inline TF_ATTRIBUTE_ALWAYS_INLINE FilterTile(_T const* input,
                                               SYCLOutputWindow const& w,
                                               int const n_cols,
                                               int const n_features) {
    input += w.offset;
    for (int r = 0; r < R; ++r) {
      for (int c = 0; c < S; ++c) {
        int idx = (r * n_cols + c) * n_features;
        data[r][c] =
            (r >= w.rsize || c >= w.csize) ? static_cast<_T>(0) : input[idx];
      }
    }
  }
};
/**
 * Base class for the output tile which provides a correctly sized data array.
 */
template <typename T, int M, int N, int R, int S>
struct BaseTransformedFilterTile {
  static constexpr int A = M + R - 1;
  static constexpr int B = N + S - 1;
  T data[A][B];
};
/**
 * Tile which transforms the intermediate layer into the output layer. Should be
 * specialised for each Winograd tranform.
 *
 * This object needs to provide the following constructor:
 *
 *   template <bool mirror>
 *   TransformedFilterTile(FilterTile<T, 2, 2, 3, 3, mirror> const& filter)
 *       : BaseTransformedFilterTile<T, 2, 2, 3, 3>{} {
 *     // Implement the filter Winograd transform
 *   }
 *
 * and needs to provide access to the base class data array with:
 *
 *   using BaseTransformedFilterTile<T, 2, 2, 3, 3>::data;
 */
template <typename T, int M, int N, int R, int S>
struct TransformedFilterTile;
/**
 * Base class for the output tile which provides a correctly sized data array.
 */
template <typename T, int M, int N, int R, int S>
struct BaseTransformedInputTile {
  static constexpr int A = M + R - 1;
  static constexpr int B = N + S - 1;
  T data[A][B];
};
/**
 * Tile which transforms the intermediate layer into the output layer. Should be
 * specialised for each Winograd tranform.
 *
 * This object needs to provide the following constructor:
 *
 *   TransformedInputTile(InputTile<T, 2, 2, 3, 3> const& inp)
 *       : BaseTransformedInputTile<T, 2, 2, 3, 3>{} {
 *     // Implement the input Winograd transform
 *   }
 *
 * and needs to provide access to the base class data array with:
 *
 *   using BaseTransformedInpuTile<T, 2, 2, 3, 3>::data;
 */
template <typename T, int M, int N, int R, int S>
struct TransformedInputTile;
/**
 * Tile to store the intermediate Winograd data. Provides an update method to
 * increment the tile with provided transformed inputs and filters.
 */
template <typename T, int M, int N, int R, int S>
struct IntermediateTile {
  static constexpr int A = M + R - 1;
  static constexpr int B = N + S - 1;
  inline TF_ATTRIBUTE_ALWAYS_INLINE IntermediateTile() {
    for (int i = 0; i < A; ++i) {
      for (int j = 0; j < B; ++j) {
        data[i][j] = static_cast<T>(0);
      }
    }
  }
  /**
   * Read the intermediate tile from a temporary buffer. The input shape is
   * expected to be
   *   [ (M+R-1)(N+S-1), (batch * tile_rows * tile_cols), features ].
   */
  template <typename _T>
  inline TF_ATTRIBUTE_ALWAYS_INLINE IntermediateTile(_T* input,
                                                     int const tile_idx,
                                                     int const n_tiles,
                                                     int const feature,
                                                     int const n_features) {
    input += tile_idx * n_features + feature;
    for (int r = 0; r < A; ++r) {
      for (int c = 0; c < B; ++c) {
        const int idx = (r * B + c) * n_features * n_tiles;
        data[r][c] = input[idx];
#if 0
        data[r][c] = *input;
        input += n_features * n_tiles;
#endif
      }
    }
  }
  inline TF_ATTRIBUTE_ALWAYS_INLINE void update(
      TransformedInputTile<T, M, N, R, S> const& inp,
      TransformedFilterTile<T, M, N, R, S> const& filter) {
    for (int r = 0; r < A; ++r) {
      for (int c = 0; c < B; ++c) {
        data[r][c] += inp.data[r][c] * filter.data[r][c];
      }
    }
  }
  T data[A][B];
};
/**
 * Base class for the output tile which provides a correctly sized data array.
 */
template <typename T, int M, int N, int R, int S>
struct BaseOutputTile {
  T data[M][N];
};
/**
 * Tile which transforms the intermediate layer into the output layer. Should be
 * specialised for each Winograd tranform.
 *
 * This object needs to provide the following constructor:
 *
 *   OutputTile(IntermediateTile<T, M, N, R, S> const& tile)
 *       : BaseOutputTile<T, M, N, R, S>{} {
 *     // Implement the inverse Winograd transform
 *   }
 *
 * and needs to provide access to the base class data array with:
 *
 *   using BaseOutputTile<T, M, N, R, S>::data;
 */
template <typename T, int M, int N, int R, int S>
struct OutputTile;
template <typename T, int M, int N, int R, int S>
struct OutputData {
  static constexpr int A = M + R - 1;
  static constexpr int B = N + S - 1;
  using Index = int;
  /**
   * Write the transformed input tile to a temporary buffer where each entry of
   * the tile is split into separate matrices. The output pointer should be at
   * the start of the temporary buffer.
   *
   * The resulting temporary buffer will be written as a batch of these
   * matrices, with a shape of
   *   [ (M+R-1)*(N+S-1), (batch * row_tiles * col_tiles), channels ].
   *
   * NOTE: The template here allows different address space attributes to be
   * passed with the pointer, rather than specifying the pointer will be to
   * global memory or to local memory.
   */
  template <typename _T>
  inline TF_ATTRIBUTE_ALWAYS_INLINE static void write_transformed_input(
      _T* output, Index const tile_idx, Index const channel,
      Index const n_tiles, Index const n_channels,
      TransformedInputTile<T, M, N, R, S> const& tile) {
    output += tile_idx * n_channels + channel;
    for (int r = 0; r < A; ++r) {
      for (int c = 0; c < B; ++c) {
        const int idx = (r * B + c) * n_tiles * n_channels;
        output[idx] = tile.data[r][c];
#if 0
        *output = tile.data[r][c];
        output += n_tiles * n_channels;
#endif
      }
    }
  }
  /**
   * Write the transformed filter tile to a temporary buffer where each entry of
   * the tile is split into separate matrices. The output pointer should be at
   * the start of the temporary buffer.
   *
   * The resulting temporary buffer will be written as a batch of these
   * matrices, with a shape of
   *   [ (M+R-1)*(N+S-1), features, channels ].
   *
   * NOTE: The template here allows different address space attributes to be
   * passed with the pointer, rather than specifying the pointer will be to
   * global memory or to local memory.
   */
  template <typename _T>
  inline TF_ATTRIBUTE_ALWAYS_INLINE static void write_transformed_filter(
      _T* output, Index const channel, Index const feature,
      Index const n_channels, Index const n_features,
      TransformedFilterTile<T, M, N, R, S> const& tile) {
    output += feature * n_channels + channel;
    for (int r = 0; r < A; ++r) {
      for (int c = 0; c < B; ++c) {
        const int idx = (r * B + c) * n_features * n_channels;
        output[idx] = tile.data[r][c];
#if 0
        *output = tile.data[r][c];
        output += n_features * n_channels;
#endif
      }
    }
  }
  /**
   * Write the output tile to the correct output memory. The output pointer
   * should be at the start of the output buffer. The resulting output shape is
   * NHWC.
   *
   * NOTE: The template here allows different address space attributes to be
   * passed with the pointer, rather than specifying the pointer will be to
   * global memory or to local memory.
   */
  template <typename _T>
  inline TF_ATTRIBUTE_ALWAYS_INLINE static void write_output(
      _T* output, SYCLOutputWindow const& window, Index const n_cols,
      Index const n_channels, OutputTile<T, M, N, R, S> const& tile) {
    output += window.offset;
    for (int r = 0; r < M && r < window.rsize; ++r) {
      for (int c = 0; c < N && c < window.csize; ++c) {
        output[(r * n_cols + c) * n_channels] = tile.data[r][c];
      }
    }
  }
  /**
   * Write the output tile to the correct output memory. The output pointer
   * should be at the start of the output buffer. The resulting output shape is
   * HWCF.
   *
   * The filter has size M x N when run in FilterBackprop mode, so we don't need
   * to check the bounds for writing to the output.
   *
   * NOTE: The template here allows different address space attributes to be
   * passed with the pointer, rather than specifying the pointer will be to
   * global memory or to local memory.
   */
  template <typename _T>
  inline TF_ATTRIBUTE_ALWAYS_INLINE static void write_filter_output(
      _T* output, Index const channel, Index const feature,
      Index const n_channels, Index const n_features,
      OutputTile<T, M, N, R, S> const& tile) {
    output += channel * n_features + feature;
    for (int r = 0; r < M; ++r) {
      for (int c = 0; c < N; ++c) {
        const int idx = (r * N + c) * n_channels * n_features;
        output[idx] = tile.data[r][c];
#if 0
        *output = tile.data[r][c];
        output += n_channels * n_features;
#endif
      }
    }
  }
};
template <typename T, int M, int N, int R, int S, ConvType CType>
struct ExtractInputTiles {
  using Index = int;
  using buffer_data = uint8_t;
  static constexpr auto read_mode = cl::sycl::access::mode::read;
  static constexpr auto write_mode = cl::sycl::access::mode::discard_write;
  static constexpr auto global_access = cl::sycl::access::target::global_buffer;
  using write_accessor =
      cl::sycl::accessor<buffer_data, 1, write_mode, global_access>;
  using read_accessor =
      cl::sycl::accessor<buffer_data, 1, read_mode, global_access>;

  inline TF_ATTRIBUTE_ALWAYS_INLINE ExtractInputTiles(
      Index const n_threads, Index const n_tiles,
      SYCLConv2DParams const& params, read_accessor const input,
      write_accessor output)
      : n_threads_{n_threads},
        n_tiles_{n_tiles},
        n_tile_rows_{RoundRatioUpAboveZero(params.out_rows_, M)},
        n_tile_cols_{RoundRatioUpAboveZero(params.out_cols_, N)},
        p_{params},
        input_accessor_{input},
        output_accessor_{output} {}

  inline TF_ATTRIBUTE_ALWAYS_INLINE void operator()(cl::sycl::item<1> item) {
    Index index = item.get(0);
    if (index < n_threads_) {
      const T* input_data = ConvertToActualTypeSycl(T, input_accessor_);
      T* output_data = ConvertToActualTypeSycl(T, output_accessor_);

      const Index channel_idx = index % p_.channels_;
      const Index tile_idx = index / p_.channels_;

      const Index brc_idx = tile_idx;
      const Index br_idx = brc_idx / n_tile_cols_;
      const Index col_idx = brc_idx % n_tile_cols_;
      const Index cstart = col_idx * N - p_.pad_cols_;
      const Index cend = cl::sycl::min(cstart + N + S - 1, p_.in_cols_);
      const Index firstc = cstart < 0 ? -cstart : 0;

      const Index batch = br_idx / n_tile_rows_;
      const Index row_idx = br_idx % n_tile_rows_;
      const Index rstart = row_idx * M - p_.pad_rows_;
      const Index rend = cl::sycl::min(rstart + M + R - 1, p_.in_rows_);
      const Index firstr = rstart < 0 ? -rstart : 0;

      const Index offset =
          ((batch * p_.in_rows_ + rstart) * p_.in_cols_ + cstart) *
              p_.channels_ +
          channel_idx;
      InputTile<T, M, N, R, S> inp(input_data + offset, p_.in_cols_,
                                   p_.channels_, rend - rstart, cend - cstart,
                                   firstr, firstc);

      OutputData<T, M, N, R, S>::write_transformed_input(
          output_data, tile_idx, channel_idx, n_tiles_, p_.channels_,
          TransformedInputTile<T, M, N, R, S>{inp});
    }
  }

 private:
  const Index n_threads_;
  const Index n_tiles_;
  const Index n_tile_rows_;
  const Index n_tile_cols_;
  const SYCLConv2DParams p_;
  const read_accessor input_accessor_;
  write_accessor output_accessor_;
};
template <typename T, int M, int N, int R, int S>
struct ExtractInputTiles<T, M, N, R, S, ConvType::FilterBackprop> {
  using Index = int;
  using buffer_data = uint8_t;
  static constexpr auto read_mode = cl::sycl::access::mode::read;
  static constexpr auto write_mode = cl::sycl::access::mode::discard_write;
  static constexpr auto global_access = cl::sycl::access::target::global_buffer;
  using write_accessor =
      cl::sycl::accessor<buffer_data, 1, write_mode, global_access>;
  using read_accessor =
      cl::sycl::accessor<buffer_data, 1, read_mode, global_access>;

  inline TF_ATTRIBUTE_ALWAYS_INLINE ExtractInputTiles(
      Index const n_threads, Index const n_tile_rows, Index const n_tile_cols,
      SYCLConv2DParams const& params, read_accessor const input,
      write_accessor output)
      : n_threads_{n_threads},
        n_tile_rows_{n_tile_rows},
        n_tile_cols_{n_tile_cols},
        p_{params},
        input_accessor_{input},
        output_accessor_{output} {}

  inline TF_ATTRIBUTE_ALWAYS_INLINE void operator()(cl::sycl::item<1> item) {
    Index index = item.get(0);
    if (index < n_threads_) {
      const T* input_data = ConvertToActualTypeSycl(T, input_accessor_);
      T* output_data = ConvertToActualTypeSycl(T, output_accessor_);

      const Index channel_idx = index % p_.channels_;
      const Index tile_idx = index / p_.channels_;

      Index batch = tile_idx;
      const Index cstart = (batch % n_tile_cols_) * S - p_.pad_cols_;
      const Index cend = std::min(cstart + N + S - 1, p_.in_cols_);
      const Index firstc = cstart < 0 ? -cstart : 0;
      batch /= n_tile_cols_;

      const Index rstart = (batch % n_tile_rows_) * R - p_.pad_rows_;
      const Index rend = std::min(rstart + M + R - 1, p_.in_rows_);
      const Index firstr = rstart < 0 ? -rstart : 0;
      batch /= n_tile_rows_;

      const Index offset =
          ((batch * p_.in_rows_ + rstart) * p_.in_cols_ + cstart) *
              p_.channels_ +
          channel_idx;
      InputTile<T, M, N, R, S> inp(input_data + offset, p_.in_cols_,
                                   p_.channels_, rend - rstart, cend - cstart,
                                   firstr, firstc);
      TransformedInputTile<T, M, N, R, S> trans{inp};

      const Index n_tiles = p_.batch_ * n_tile_rows_ * n_tile_cols_;
      OutputData<T, M, N, R, S>::write_transformed_input(
          output_data, tile_idx, channel_idx, n_tiles, p_.channels_, trans);
    }
  }

 private:
  const Index n_threads_;
  const Index n_tile_rows_;
  const Index n_tile_cols_;
  const SYCLConv2DParams p_;
  const read_accessor input_accessor_;
  write_accessor output_accessor_;
};
template <typename T, int M, int N, int R, int S, ConvType CType>
struct ExtractKernelTiles {
  using Index = int;
  using buffer_data = uint8_t;
  static constexpr auto read_mode = cl::sycl::access::mode::read;
  static constexpr auto write_mode = cl::sycl::access::mode::discard_write;
  static constexpr auto global_access = cl::sycl::access::target::global_buffer;
  using write_accessor =
      cl::sycl::accessor<buffer_data, 1, write_mode, global_access>;
  using read_accessor =
      cl::sycl::accessor<buffer_data, 1, read_mode, global_access>;

  inline TF_ATTRIBUTE_ALWAYS_INLINE ExtractKernelTiles(
      Index const n_threads, Index const /*n_tiles*/,
      SYCLConv2DParams const& params, read_accessor const kernel,
      write_accessor output)
      : n_threads_{n_threads},
        n_channels_{params.channels_},
        n_features_{params.features_},
        kernel_accessor_{kernel},
        output_accessor_{output} {}

  inline TF_ATTRIBUTE_ALWAYS_INLINE void operator()(cl::sycl::item<1> item) {
    Index index = item.get(0);
    if (index < n_threads_) {
      const T* kernel_data = ConvertToActualTypeSycl(T, kernel_accessor_);
      T* output_data = ConvertToActualTypeSycl(T, output_accessor_);

      const Index feature_idx = index % n_features_;
      const Index channel_idx = index / n_features_;

      FilterTile<T, M, N, R, S, CType> filter(
          kernel_data, channel_idx, feature_idx, n_channels_, n_features_);
      TransformedFilterTile<T, M, N, R, S> transformed{filter};

      OutputData<T, M, N, R, S>::write_transformed_filter(
          output_data, channel_idx, feature_idx, n_channels_, n_features_,
          transformed);
    }
  }

 private:
  const Index n_threads_;
  const Index n_channels_;
  const Index n_features_;
  const read_accessor kernel_accessor_;
  write_accessor output_accessor_;
};
template <typename T, int M, int N, int R, int S>
struct ExtractKernelTiles<T, M, N, R, S, ConvType::InputBackprop> {
  using Index = int;
  using buffer_data = uint8_t;
  static constexpr auto CType = ConvType::InputBackprop;
  static constexpr auto read_mode = cl::sycl::access::mode::read;
  static constexpr auto write_mode = cl::sycl::access::mode::discard_write;
  static constexpr auto global_access = cl::sycl::access::target::global_buffer;
  using write_accessor =
      cl::sycl::accessor<buffer_data, 1, write_mode, global_access>;
  using read_accessor =
      cl::sycl::accessor<buffer_data, 1, read_mode, global_access>;

  /*
   * Note that for the input backprop the features and channels in params have
   * been switched. params.channels_ are most packed in memory, which we expect
   * to be n_features_ in the kernel. We switch these back in the constructor
   * so they are as expected.
   */
  inline TF_ATTRIBUTE_ALWAYS_INLINE ExtractKernelTiles(
      Index const n_threads, Index const /*n_tiles*/,
      SYCLConv2DParams const& params, read_accessor const kernel,
      write_accessor output)
      : n_threads_{n_threads},
        n_features_{params.channels_},
        n_channels_{params.features_},
        kernel_accessor_{kernel},
        output_accessor_{output} {}

  inline TF_ATTRIBUTE_ALWAYS_INLINE void operator()(cl::sycl::item<1> item) {
    Index index = item.get(0);
    if (index < n_threads_) {
      const T* kernel_data = ConvertToActualTypeSycl(T, kernel_accessor_);
      T* output_data = ConvertToActualTypeSycl(T, output_accessor_);

      const Index feature_idx = index / n_channels_;
      const Index channel_idx = index % n_channels_;

      FilterTile<T, M, N, R, S, CType> filter(
          kernel_data, channel_idx, feature_idx, n_channels_, n_features_);
      TransformedFilterTile<T, M, N, R, S> transformed{filter};
      /*
       * Here we can either write out with features or channels packed in
       * memory. The matmul will perform best if the features are packed, as
       * then the matmul will read this in a packed way, if instead the channels
       * are packed then the matmul will require a transpose which may affect
       * performance. The problem is that in this kernel we read the data with
       * channels packed, so writing out with features packed will require a
       * large number of strided writes.
       *
       * Here we write out with channels packed, and so the matmul must be
       * called with this tensor transposed.
       */
      OutputData<T, M, N, R, S>::write_transformed_filter(
          output_data, channel_idx, feature_idx, n_channels_, n_features_,
          transformed);
    }
  }

 private:
  const Index n_threads_;
  const Index n_features_;
  const Index n_channels_;
  const read_accessor kernel_accessor_;
  write_accessor output_accessor_;
};
template <typename T, int M, int N, int R, int S>
struct ExtractKernelTiles<T, M, N, R, S, ConvType::FilterBackprop> {
  using Index = int;
  using buffer_data = uint8_t;
  static constexpr auto CType = ConvType::FilterBackprop;
  static constexpr auto read_mode = cl::sycl::access::mode::read;
  static constexpr auto write_mode = cl::sycl::access::mode::discard_write;
  static constexpr auto global_access = cl::sycl::access::target::global_buffer;
  using write_accessor =
      cl::sycl::accessor<buffer_data, 1, write_mode, global_access>;
  using read_accessor =
      cl::sycl::accessor<buffer_data, 1, read_mode, global_access>;

  inline TF_ATTRIBUTE_ALWAYS_INLINE ExtractKernelTiles(
      Index const n_threads, Index const n_tiles,
      SYCLConv2DParams const& params, read_accessor const kernel,
      write_accessor output)
      : n_threads_{n_threads},
        n_tiles_{n_tiles},
        p_{params},
        kernel_accessor_{kernel},
        output_accessor_{output} {}

  inline TF_ATTRIBUTE_ALWAYS_INLINE void operator()(cl::sycl::item<1> item) {
    Index index = item.get(0);
    if (index < n_threads_) {
      const T* kernel_data = ConvertToActualTypeSycl(T, kernel_accessor_);
      T* output_data = ConvertToActualTypeSycl(T, output_accessor_);

      const Index feature = index % p_.features_;
      const Index tile_idx = index / p_.features_;

      SYCLOutputWindow w =
          p_.winograd_kernel_from_tile<M, N, R, S>(tile_idx, feature);

      FilterTile<T, M, N, R, S, CType> filter(kernel_data, w, p_.window_cols_,
                                              p_.features_);
      TransformedFilterTile<T, M, N, R, S> transformed{filter};

      OutputData<T, M, N, R, S>::write_transformed_filter(
          output_data, feature, tile_idx, p_.features_, n_tiles_, transformed);
    }
  }

 private:
  const Index n_threads_;
  const Index n_tiles_;
  const SYCLConv2DParams p_;
  const read_accessor kernel_accessor_;
  write_accessor output_accessor_;
};
template <typename T, int M, int N, int R, int S, ConvType CType>
struct ExtractOutputTiles {
  using Index = int;
  using buffer_data = uint8_t;
  static constexpr auto read_mode = cl::sycl::access::mode::read;
  static constexpr auto write_mode = cl::sycl::access::mode::discard_write;
  static constexpr auto global_access = cl::sycl::access::target::global_buffer;
  using write_accessor =
      cl::sycl::accessor<buffer_data, 1, write_mode, global_access>;
  using read_accessor =
      cl::sycl::accessor<buffer_data, 1, read_mode, global_access>;

  inline TF_ATTRIBUTE_ALWAYS_INLINE ExtractOutputTiles(
      Index const n_threads, Index const n_tiles,
      SYCLConv2DParams const& params, read_accessor const input,
      write_accessor output)
      : n_threads_{n_threads},
        n_tiles_{n_tiles},
        n_tile_rows_{RoundRatioUpAboveZero(params.out_rows_, M)},
        n_tile_cols_{RoundRatioUpAboveZero(params.out_cols_, N)},
        p_{params},
        input_accessor_{input},
        output_accessor_{output} {}

  inline TF_ATTRIBUTE_ALWAYS_INLINE void operator()(cl::sycl::item<1> item) {
    Index index = item.get(0);
    if (index < n_threads_) {
      const T* input_data = ConvertToActualTypeSycl(T, input_accessor_);
      T* output_data = ConvertToActualTypeSycl(T, output_accessor_);

      const Index feature = index % p_.features_;
      const Index tile_idx = index / p_.features_;

      IntermediateTile<T, M, N, R, S> tmp{input_data, tile_idx, n_tiles_,
                                          feature, p_.features_};

      const Index brc_idx = tile_idx;
      const Index br_idx = brc_idx / n_tile_cols_;
      const Index col_idx = brc_idx % n_tile_cols_;
      const Index col = col_idx * N;
      const Index cend = cl::sycl::min(col + N, p_.out_cols_);

      const Index batch = br_idx / n_tile_rows_;
      const Index row_idx = br_idx % n_tile_rows_;
      const Index row = row_idx * M;
      const Index rend = cl::sycl::min(row + M, p_.out_rows_);

      const Index offset =
          ((batch * p_.out_rows_ + row) * p_.out_cols_ + col) * p_.features_ +
          feature;

      SYCLOutputWindow out_w{rend - row, cend - col, offset};

      OutputData<T, M, N, R, S>::write_output(output_data, out_w, p_.out_cols_,
                                              p_.features_,
                                              OutputTile<T, M, N, R, S>{tmp});
    }
  }

 private:
  const Index n_threads_;
  const Index n_tiles_;
  const Index n_tile_rows_;
  const Index n_tile_cols_;
  const SYCLConv2DParams p_;
  const read_accessor input_accessor_;
  write_accessor output_accessor_;
};
template <typename T, int M, int N, int R, int S>
struct ExtractOutputTiles<T, M, N, R, S, ConvType::FilterBackprop> {
  using Index = int;
  using buffer_data = uint8_t;
  static constexpr auto read_mode = cl::sycl::access::mode::read;
  static constexpr auto write_mode = cl::sycl::access::mode::discard_write;
  static constexpr auto global_access = cl::sycl::access::target::global_buffer;
  using write_accessor =
      cl::sycl::accessor<buffer_data, 1, write_mode, global_access>;
  using read_accessor =
      cl::sycl::accessor<buffer_data, 1, read_mode, global_access>;

  inline TF_ATTRIBUTE_ALWAYS_INLINE ExtractOutputTiles(
      Index const n_threads, Index const /*n_tiles*/,
      SYCLConv2DParams const& params, read_accessor const input,
      write_accessor output)
      : n_threads_{n_threads},
        p_{params},
        input_accessor_{input},
        output_accessor_{output} {}

  inline TF_ATTRIBUTE_ALWAYS_INLINE void operator()(cl::sycl::item<1> item) {
    Index index = item.get(0);
    if (index < n_threads_) {
      const T* input_data = ConvertToActualTypeSycl(T, input_accessor_);
      T* output_data = ConvertToActualTypeSycl(T, output_accessor_);

      const Index feature = index % p_.features_;
      const Index channel = index / p_.features_;

      IntermediateTile<T, M, N, R, S> tmp{input_data, channel, p_.channels_,
                                          feature, p_.features_};
      OutputData<T, M, N, R, S>::write_filter_output(
          output_data, channel, feature, p_.channels_, p_.features_,
          OutputTile<T, M, N, R, S>{tmp});
    }
  }

 private:
  const Index n_threads_;
  const SYCLConv2DParams p_;
  const read_accessor input_accessor_;
  write_accessor output_accessor_;
};
}  // namespace winograd
}  // namespace tensorflow
#endif  // TENSORFLOW_KERNELS_CONV_OPS_WINOGRAD_SYCL_KERNELS_H_
